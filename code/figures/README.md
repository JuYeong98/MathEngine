Data and code for generating Figures 4, 5, and 7 in the paper:

Figure 4: Comparison of the automatic solve rates on (A) MIT math courses and a Columbia University course, and on (B) MATH benchmark dataset. The latest OpenAI GPT-3 (text-davinci-002), a transformer pre-trained on text, achieves on the MIT math courses (A) 18.8% with zero-shot and 30.8% with CoT, and on the MATH benchmark (B) 25.5% with zero-shot and 42.2% with CoT. In contrast, program synthesis with zero-shot and few-shot learning using the latest OpenAI Codex (code-davinci-002), a transformer pre-trained on text and fine tuned on code, achieves automatic solve rates of 80% (A) and 81.1% (B).

Figure 5: Imported Python programming libraries by course: NumPy is used by nearly
all courses. Matplotlib is used in courses with questions that involve plotting. Sympy
is used by most of the courses, and SciPy by half of the courses.

Figure 7: Student survey results: Panel A compares the level of difficulty of human-written questions and questions generated by our approach for each course based on the student ratings. The plot shows the means of the difficulty ratings between 1 (easiest) and 5 (hardest) and their 95% confidence intervals. Panel B shows the percentage of human-written and machine-generated questions rated as appropriate and not appropriate for the course. Panel C shows the percentage of human-written questions rated as human-written or machine-generated (left) and the percentage of machine-generated questions rated as human-written or machine-generated (right).
