Code for generating Figures 4, 5, and 7:

Figure 4: Comparison of the automatic solve rates on MIT math courses and a Columbia University course (A), and a MATH benchmark dataset (B). The latest OpenAI GPT-3 (text-davinci-002), a transformer pre-trained on text, achieves $18\%$ with zero-shot and $x\%$ with CoT (A) and $25.5\%$ with zero-shot and $y\%$ with CoT (B) automatic solve rates. In contrast, program synthesis with zero-shot and few-shot learning using the latest OpenAI Codex (code-davinci-002), a transformer pre-trained on text and fine tuned on code, achieves an automatic solve rate of $80\%$ (A) and $81.1\%$ (B).

Figure 5: Imported Python programming libraries by course: NumPy is used by nearly
all courses. Matplotlib is used in courses with questions that involve plotting. Sympy
is used by most of the courses, and SciPy by half of the courses.

Figure 7: Student survey results: Panel A compares the level of difficulty of human-written questions and questions generated by our approach for each course based on the student ratings. The plot shows the means of the difficulty ratings between 1 (easiest) and 5 (hardest) and their $95\%$ confidence intervals. Panel B shows the percentage of human-written and machine-generated questions rated as appropriate and not appropriate for the course. Panel C shows the percentage of human-written questions rated as human-written or machine-generated (left) and the percentage of machine-generated questions rated as human-written or machine-generated (right).
